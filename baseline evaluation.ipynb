{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcae74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.inspection import permutation_importance\n",
    "import re\n",
    "from fractions import Fraction\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c1be9",
   "metadata": {},
   "source": [
    "# Create the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "616ef70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('home_depot_data/train.csv', encoding=\"ISO-8859-1\")\n",
    "df_test = pd.read_csv('home_depot_data/test.csv', encoding=\"ISO-8859-1\")\n",
    "df_attr = pd.read_csv('home_depot_data/attributes.csv')\n",
    "df_pro_desc = pd.read_csv('home_depot_data/product_descriptions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15600763",
   "metadata": {},
   "source": [
    "# Add stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56870a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "def str_stemmer(s):\n",
    "\treturn \" \".join([stemmer.stem(word) for word in s.lower().split()])\n",
    "\n",
    "def str_common_word(str1, str2):\n",
    "\treturn sum(int(str2.find(word)>=0) for word in str1.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064f13f5",
   "metadata": {},
   "source": [
    "# Dataframe processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "38fb4b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_base = df_train.drop(['search_term','product_title'],axis=1)\n",
    "df_train['search_term'] = df_train['search_term'].map(lambda x:str_stemmer(x))\n",
    "df_train['product_title'] = df_train['product_title'].map(lambda x:str_stemmer(x))\n",
    "df_train['len_of_query'] = df_train['search_term'].map(lambda x:len(x.split())).astype(np.int64)\n",
    "df_train['product_info'] = df_train['search_term']+\"\\t\"+df_train['product_title']\n",
    "df_train['word_in_title'] = df_train['product_info'].map(lambda x:str_common_word(x.split('\\t')[0],x.split('\\t')[1]))\n",
    "df_complete=df_train\n",
    "df_train = df_train.drop(['search_term','product_title', 'product_info'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8030299",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df_train, test_size=0.2, random_state=42)\n",
    "base_train, base_test = train_test_split(df_train_base, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093ffe48",
   "metadata": {},
   "source": [
    "# Define test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "908d4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_test = test['id']\n",
    "y_train = train['relevance'].values\n",
    "X_train = train.drop(['id','relevance'],axis=1).values\n",
    "X_test = test.drop(['id','relevance'],axis=1).values\n",
    "y_test = test['relevance'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60f963f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=15, max_depth=6, random_state=42)\n",
    "clf = BaggingRegressor(rf, n_estimators=45, max_samples=0.1, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5dcb0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_base_test = base_test['id']\n",
    "y_base_train = base_train['relevance'].values\n",
    "X_base_train = base_train.drop(['id','relevance'],axis=1).values\n",
    "X_base_test = base_test.drop(['id','relevance'],axis=1).values\n",
    "y_base_test = base_test['relevance'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0f7e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = RandomForestRegressor(n_estimators=15, max_depth=6, random_state=42)\n",
    "clf_base = BaggingRegressor(rf_base, n_estimators=45, max_samples=0.1, random_state=42)\n",
    "clf_base.fit(X_base_train, y_base_train)\n",
    "y_base_pred = clf_base.predict(X_base_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38a2dec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 0.4842\n",
      "Root Mean Squared Error (RMSE): 0.5245\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "rmse_base = np.sqrt(mean_squared_error(y_base_test, y_base_pred))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_base:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eab1aaf",
   "metadata": {},
   "source": [
    "# Improving the matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4dcd4bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         attribute_name  count\n",
      "0        MFG Brand Name  86250\n",
      "1              Bullet02  86248\n",
      "2              Bullet03  86226\n",
      "3              Bullet04  86174\n",
      "4              Bullet01  85940\n",
      "5   Product Width (in.)  61137\n",
      "6              Bullet05  60529\n",
      "7  Product Height (in.)  54698\n",
      "8   Product Depth (in.)  53652\n",
      "9  Product Weight (lb.)  45175\n"
     ]
    }
   ],
   "source": [
    "attribute_counts = df_attr['name'].value_counts().reset_index()\n",
    "attribute_counts.columns = ['attribute_name', 'count']\n",
    "\n",
    "print(attribute_counts.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1b1c0427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reinierbos/Documents/data-science-a3/venv/lib/python3.12/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m text_cols = [\u001b[33m'\u001b[39m\u001b[33msearch_term\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mproduct_title\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mall_bullets\u001b[39m\u001b[33m'\u001b[39m] + [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_value\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m selected_attrs]\n\u001b[32m     24\u001b[39m unique_texts = pd.Series(df_complete[text_cols].values.ravel()).dropna().unique()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m text2doc = \u001b[43m{\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munique_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mner\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtagger\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Similarity function\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_similarity\u001b[39m(text1, text2):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/spacy/language.py:1621\u001b[39m, in \u001b[36mLanguage.pipe\u001b[39m\u001b[34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[39m\n\u001b[32m   1619\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[32m   1620\u001b[39m         docs = pipe(docs)\n\u001b[32m-> \u001b[39m\u001b[32m1621\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1622\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/spacy/util.py:1703\u001b[39m, in \u001b[36m_pipe\u001b[39m\u001b[34m(docs, proc, name, default_error_handler, kwargs)\u001b[39m\n\u001b[32m   1693\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pipe\u001b[39m(\n\u001b[32m   1694\u001b[39m     docs: Iterable[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1695\u001b[39m     proc: \u001b[33m\"\u001b[39m\u001b[33mPipeCallable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1700\u001b[39m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1701\u001b[39m ) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1702\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1703\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m proc.pipe(docs, **kwargs)\n\u001b[32m   1704\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1705\u001b[39m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[32m   1706\u001b[39m         kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/spacy/pipeline/pipe.pyx:55\u001b[39m, in \u001b[36mpipe\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/spacy/util.py:1703\u001b[39m, in \u001b[36m_pipe\u001b[39m\u001b[34m(docs, proc, name, default_error_handler, kwargs)\u001b[39m\n\u001b[32m   1693\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pipe\u001b[39m(\n\u001b[32m   1694\u001b[39m     docs: Iterable[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1695\u001b[39m     proc: \u001b[33m\"\u001b[39m\u001b[33mPipeCallable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1700\u001b[39m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1701\u001b[39m ) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1702\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1703\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m proc.pipe(docs, **kwargs)\n\u001b[32m   1704\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1705\u001b[39m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[32m   1706\u001b[39m         kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/spacy/pipeline/pipe.pyx:55\u001b[39m, in \u001b[36mpipe\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/spacy/util.py:1703\u001b[39m, in \u001b[36m_pipe\u001b[39m\u001b[34m(docs, proc, name, default_error_handler, kwargs)\u001b[39m\n\u001b[32m   1693\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pipe\u001b[39m(\n\u001b[32m   1694\u001b[39m     docs: Iterable[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1695\u001b[39m     proc: \u001b[33m\"\u001b[39m\u001b[33mPipeCallable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1700\u001b[39m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1701\u001b[39m ) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1702\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1703\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m proc.pipe(docs, **kwargs)\n\u001b[32m   1704\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1705\u001b[39m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[32m   1706\u001b[39m         kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/spacy/pipeline/trainable_pipe.pyx:75\u001b[39m, in \u001b[36mpipe\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/spacy/pipeline/tok2vec.py:126\u001b[39m, in \u001b[36mTok2Vec.predict\u001b[39m\u001b[34m(self, docs)\u001b[39m\n\u001b[32m    124\u001b[39m     width = \u001b[38;5;28mself\u001b[39m.model.get_dim(\u001b[33m\"\u001b[39m\u001b[33mnO\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.model.ops.alloc((\u001b[32m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m tokvecs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/model.py:334\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) -> OutT:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/layers/with_array.py:42\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, Xseq, is_train)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.layers[\u001b[32m0\u001b[39m](Xseq, is_train)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/layers/with_array.py:77\u001b[39m, in \u001b[36m_list_forward\u001b[39m\u001b[34m(model, Xs, is_train)\u001b[39m\n\u001b[32m     75\u001b[39m lengths = NUMPY_OPS.asarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[32m     76\u001b[39m Xf = layer.ops.flatten(Xs, pad=pad)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m Yf, get_dXf = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dYs: ListXd) -> ListXd:\n\u001b[32m     80\u001b[39m     dYf = layer.ops.flatten(dYs, pad=pad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/layers/residual.py:41\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output + dX\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m Y, backprop_layer = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] + Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "    \u001b[31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/data-science-a3/venv/lib/python3.12/site-packages/thinc/layers/maxout.py:52\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     50\u001b[39m W = model.get_param(\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m W = model.ops.reshape2f(W, nO * nP, nI)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m Y = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m Y += model.ops.reshape1f(b, nO * nP)\n\u001b[32m     54\u001b[39m Z = model.ops.reshape3f(Y, Y.shape[\u001b[32m0\u001b[39m], nO, nP)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Preprocessing\n",
    "df_attr['name'] = df_attr['name'].fillna('').str.lower()\n",
    "df_attr['value'] = df_attr['value'].astype(str).str.lower()\n",
    "\n",
    "# Combine bullet attributes\n",
    "df_bullets = df_attr[df_attr['name'].str.startswith(\"bullet\")].copy()\n",
    "bullet_texts = df_bullets.groupby('product_uid')['value'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "bullet_texts = bullet_texts.rename(columns={'value': 'all_bullets'})\n",
    "df_complete = df_complete.merge(bullet_texts, on='product_uid', how='left')\n",
    "\n",
    "# Selected attributes\n",
    "selected_attrs = ['mfg brand name', 'material', 'color/finish', 'certifications and listings', 'all_bullets', 'product_title']\n",
    "\n",
    "# Add selected attributes to df_complete\n",
    "for attr_name in selected_attrs:\n",
    "    attr_subset = df_attr[df_attr['name'] == attr_name].drop_duplicates('product_uid')\n",
    "    attr_subset = attr_subset.rename(columns={'value': f'{attr_name}_value'})\n",
    "    df_complete = df_complete.merge(attr_subset[['product_uid', f'{attr_name}_value']], on='product_uid', how='left')\n",
    "\n",
    "# Prepare texts for spaCy processing\n",
    "text_cols = ['search_term', 'product_title', 'all_bullets'] + [f'{attr}_value' for attr in selected_attrs]\n",
    "unique_texts = pd.Series(df_complete[text_cols].values.ravel()).dropna().unique()\n",
    "text2doc = {text: doc for text, doc in zip(unique_texts, nlp.pipe(unique_texts, disable=[\"ner\", \"tagger\", \"parser\"]))}\n",
    "\n",
    "# Similarity function\n",
    "def get_similarity(text1, text2):\n",
    "    if pd.isna(text1) or pd.isna(text2):\n",
    "        return 0.0\n",
    "    doc1 = text2doc.get(text1)\n",
    "    doc2 = text2doc.get(text2)\n",
    "    if doc1 is None or doc2 is None or doc1.vector_norm == 0 or doc2.vector_norm == 0:\n",
    "        return 0.0\n",
    "    return doc1.similarity(doc2)\n",
    "\n",
    "# Compute similarities\n",
    "for attr_name in selected_attrs:\n",
    "    print(f\"Processing: {attr_name}\")\n",
    "    attr_col = attr_name if attr_name in ['all_bullets', 'product_title'] else f'{attr_name}_value'\n",
    "    sim_col = f'spacy_sim_{attr_name}'\n",
    "\n",
    "    df_complete[sim_col] = df_complete.apply(\n",
    "        lambda row: get_similarity(row['search_term'], row[attr_col]), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78051b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete['brand_match'] = df_complete.apply(\n",
    "    lambda row: int(str(row['search_term']).lower() in str(row.get('mfg brand name_value', '')).lower()), axis=1\n",
    ")\n",
    "\n",
    "def normalized_word_overlap(query, text):\n",
    "    q_words = set(query.split())\n",
    "    t_words = set(text.split())\n",
    "    if not q_words: return 0\n",
    "    return len(q_words & t_words) / len(q_words)\n",
    "\n",
    "df_complete['norm_overlap_title'] = df_complete.apply(\n",
    "    lambda row: normalized_word_overlap(row['search_term'], row['product_title']), axis=1\n",
    ")\n",
    "\n",
    "df_complete['length_diff'] = abs(\n",
    "    df_complete['search_term'].map(lambda x: len(x.split())) - \n",
    "    df_complete['product_title'].map(lambda x: len(x.split()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8065b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf_cosine(df, col_text, new_feature_name):\n",
    "    corpus = pd.concat([df['search_term'], df[col_text]]).astype(str)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(corpus)\n",
    "\n",
    "    tfidf_query = vectorizer.transform(df['search_term'].astype(str))\n",
    "    tfidf_col = vectorizer.transform(df[col_text].astype(str))\n",
    "\n",
    "    df[new_feature_name] = [\n",
    "        cosine_similarity(tfidf_query[i], tfidf_col[i])[0][0]\n",
    "        for i in range(len(df))\n",
    "    ]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = df_complete.merge(df_pro_desc, on='product_uid', how='left')\n",
    "df_complete['product_description'] = df_complete['product_description'].fillna(\"\")\n",
    "df_complete['all_bullets'] = df_complete['all_bullets'].fillna(\"\")\n",
    "\n",
    "df_complete = compute_tfidf_cosine(df_complete, 'product_title', 'cosine_title')\n",
    "df_complete = compute_tfidf_cosine(df_complete, 'product_description', 'cosine_description')\n",
    "df_complete = compute_tfidf_cosine(df_complete, 'all_bullets', 'cosine_bullets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19937192",
   "metadata": {},
   "source": [
    "# Adding numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd2e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: assembled height (in.)\n",
      "Processing: assembled width (in.)\n",
      "Processing: assembled depth (in.)\n",
      "Processing: product length (in.)\n",
      "Processing: product width (in.)\n",
      "Processing: product height (in.)\n",
      "Processing: product depth (in.)\n",
      "Processing: product weight (lb.)\n"
     ]
    }
   ],
   "source": [
    "num_attrs = [\n",
    "    'assembled height (in.)',\n",
    "    'assembled width (in.)',\n",
    "    'assembled depth (in.)',\n",
    "    'product length (in.)',\n",
    "    'product width (in.)',\n",
    "    'product height (in.)',\n",
    "    'product depth (in.)',\n",
    "    'product weight (lb.)'\n",
    "]\n",
    "\n",
    "for attr_name in num_attrs:\n",
    "    print(f\"Processing: {attr_name}\")\n",
    "    \n",
    "    attr_subset = df_attr[df_attr['name'] == attr_name].drop_duplicates('product_uid')\n",
    "    \n",
    "    if attr_subset.empty:\n",
    "        print(f\"⚠️ Attribute '{attr_name}' not found. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    col_name = f'{attr_name}_value'\n",
    "    if col_name in df_complete.columns:\n",
    "        df_complete.drop(columns=[col_name], inplace=True)\n",
    "\n",
    "    attr_subset = attr_subset.rename(columns={'value': col_name})\n",
    "    df_complete = df_complete.merge(attr_subset[['product_uid', col_name]], on='product_uid', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cea873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numeric(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    val = str(val).lower().strip()\n",
    "    \n",
    "    if '-' in val:\n",
    "        parts = val.split('-')\n",
    "        try:\n",
    "            return float(parts[0]) + float(Fraction(parts[1]))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    try:\n",
    "        return float(Fraction(val))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', val)\n",
    "    if match:\n",
    "        return float(match.group(1))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attr in num_attrs:\n",
    "    col = f\"{attr}_value\"\n",
    "    num_col = f\"{attr}_num\"\n",
    "    df_complete[num_col] = df_complete[col].apply(extract_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58897159",
   "metadata": {},
   "source": [
    "# Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3bcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE with improved features: 0.4831\n",
      "Baseline RMSE: 0.4842\n"
     ]
    }
   ],
   "source": [
    "spacy_features = [col for col in df_complete.columns if col.startswith('spacy_sim_')]\n",
    "numeric_attr_features = [f\"{attr}_num\" for attr in num_attrs]\n",
    "\n",
    "improved = df_complete[\n",
    "    ['id', 'relevance', 'cosine_title', \"cosine_description\", \"cosine_bullets\", 'len_of_query',\n",
    "     'word_in_title', \"brand_match\", \"norm_overlap_title\", \"length_diff\"]\n",
    "    + spacy_features\n",
    "]\n",
    "\n",
    "improved_num = df_complete[\n",
    "    ['id', 'relevance', 'cosine_title', \"cosine_description\", \"cosine_bullets\", 'len_of_query',\n",
    "     'word_in_title', \"brand_match\", \"norm_overlap_title\", \"length_diff\"]\n",
    "    + spacy_features\n",
    "    + numeric_attr_features\n",
    "]\n",
    "\n",
    "train_idx = train.index\n",
    "test_idx = test.index\n",
    "\n",
    "X_improved_train = improved.loc[train_idx].drop(['id', 'relevance'], axis=1)\n",
    "X_improved_test = improved.loc[test_idx].drop(['id', 'relevance'], axis=1)\n",
    "y_improved_train = improved.loc[train_idx]['relevance'].values\n",
    "y_improved_test = improved.loc[test_idx]['relevance'].values\n",
    "\n",
    "X_improved_train_num = improved_num.loc[train_idx].drop(['id', 'relevance'], axis=1)\n",
    "X_improved_test_num = improved_num.loc[test_idx].drop(['id', 'relevance'], axis=1)\n",
    "y_improved_train_num = improved_num.loc[train_idx]['relevance'].values\n",
    "y_improved_test_num = improved_num.loc[test_idx]['relevance'].values\n",
    "\n",
    "rf_improved = RandomForestRegressor(n_estimators=15, max_depth=6, random_state=42)\n",
    "clf_improved = BaggingRegressor(rf_improved, n_estimators=45, max_samples=0.1, random_state=42)\n",
    "clf_improved.fit(X_improved_train, y_improved_train)\n",
    "\n",
    "y_improved_pred = clf_improved.predict(X_improved_test)\n",
    "rmse_improved = np.sqrt(mean_squared_error(y_improved_test, y_improved_pred))\n",
    "\n",
    "print(f\"RMSE with improved features: {rmse_improved:.4f}\")\n",
    "print(f\"Baseline RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f76728c",
   "metadata": {},
   "source": [
    "# Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506d787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest + Bagging    | RMSE: 0.4831 | Time: 6.26s\n",
      "Gradient Boosting         | RMSE: 0.4784 | Time: 11.55s\n",
      "HistGradientBoosting      | RMSE: 0.4788 | Time: 0.33s\n",
      "SVR                       | RMSE: 0.4931 | Time: 43.94s\n",
      "KNN                       | RMSE: 0.5211 | Time: 0.17s\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"RandomForest + Bagging\": clf_improved,\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=42),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingRegressor(max_iter=100, max_depth=4, random_state=42),    \n",
    "    \"SVR\": SVR(C=1.0, epsilon=0.2),\n",
    "    \"KNN\": KNeighborsRegressor(n_neighbors=5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    start = time.time()\n",
    "    model.fit(X_improved_train, y_improved_train)\n",
    "    preds = model.predict(X_improved_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_improved_test, preds))\n",
    "    duration = time.time() - start\n",
    "    results[name] = (rmse, duration)\n",
    "\n",
    "for name, (rmse, time_taken) in results.items():\n",
    "    print(f\"{name:<25} | RMSE: {rmse:.4f} | Time: {time_taken:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c9eeb7",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization - Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba1ad2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best Gradient Boosting RMSE after tuning: 0.4764\n",
      "Best Parameters: {'subsample': 0.6, 'n_estimators': 200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_depth': 6, 'learning_rate': 0.05}\n"
     ]
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=60, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "random_search.fit(X_improved_train, y_improved_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "y_best_pred = best_model.predict(X_improved_test)\n",
    "rmse_best = np.sqrt(mean_squared_error(y_improved_test, y_best_pred))\n",
    "\n",
    "print(f\"Best Gradient Boosting RMSE after tuning: {rmse_best:.4f}\")\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50407da",
   "metadata": {},
   "source": [
    "# Model with numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cb7eec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n",
      "Best HistGradientBoosting RMSE after tuning: 0.4721\n",
      "Best Parameters: {'min_samples_leaf': 30, 'max_iter': 300, 'max_depth': None, 'max_bins': 255, 'learning_rate': 0.1, 'l2_regularization': 0.0, 'early_stopping': False}\n"
     ]
    }
   ],
   "source": [
    "# HistGradientBoosting because of NaN values\n",
    "\n",
    "param_distributions = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_iter': [100, 200, 300],\n",
    "    'max_depth': [None, 3, 5, 7],\n",
    "    'min_samples_leaf': [10, 20, 30],\n",
    "    'l2_regularization': [0.0, 0.1, 0.5, 1.0],\n",
    "    'max_bins': [128, 255], \n",
    "    'early_stopping': [True, False]\n",
    "}\n",
    "\n",
    "hgb = HistGradientBoostingRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=hgb,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=60,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "random_search.fit(X_improved_train_num, y_improved_train_num)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "y_best_pred = best_model.predict(X_improved_test_num)\n",
    "rmse_best = np.sqrt(mean_squared_error(y_improved_test_num, y_best_pred))\n",
    "\n",
    "print(f\"Best HistGradientBoosting RMSE after tuning: {rmse_best:.4f}\")\n",
    "print(\"Best Parameters:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44a6faf",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b951ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most important features:\n",
      "              feature  importance_mean  importance_std\n",
      "6  norm_overlap_title         0.020290        0.000694\n",
      "3        len_of_query         0.019772        0.000665\n",
      "4       word_in_title         0.018924        0.000882\n",
      "0        cosine_title         0.016355        0.000760\n",
      "1  cosine_description         0.009452        0.000594\n"
     ]
    }
   ],
   "source": [
    "# Compute permutation importance\n",
    "result = permutation_importance(\n",
    "    estimator=best_model,\n",
    "    X=X_improved_test_num,\n",
    "    y=y_improved_test_num,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    scoring='neg_root_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Convert to DataFrame\n",
    "importances_df = pd.DataFrame({\n",
    "    'feature': X_improved_test_num.columns,\n",
    "    'importance_mean': result.importances_mean,\n",
    "    'importance_std': result.importances_std\n",
    "}).sort_values(by='importance_mean', ascending=False)\n",
    "\n",
    "# Show top 5\n",
    "print(\"Top 5 most important features:\")\n",
    "print(importances_df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9714697a",
   "metadata": {},
   "source": [
    "# Compare feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa96e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reinierbos/Documents/data-science-a3/venv/lib/python3.12/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:19: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy: RMSE = 0.5169\n",
      "Cosine: RMSE = 0.4958\n",
      "General: RMSE = 0.4886\n",
      "Numeric: RMSE = 0.5188\n",
      "All: RMSE = 0.4747\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# --- Define your feature groups ---\n",
    "spacy_features = [col for col in df_complete.columns if col.startswith('spacy_sim_')]\n",
    "cosine_features = ['cosine_title', 'cosine_description', 'cosine_bullets']\n",
    "general_features = ['norm_overlap_title', 'word_in_title', 'brand_match', 'length_diff']\n",
    "numeric_features = [f\"{attr}_num\" for attr in num_attrs] + ['len_of_query']\n",
    "\n",
    "feature_sets = {\n",
    "    \"SpaCy\": spacy_features,\n",
    "    \"Cosine\": cosine_features,\n",
    "    \"General\": general_features,\n",
    "    \"Numeric\": numeric_features,\n",
    "    \"All\": spacy_features + cosine_features + general_features + numeric_features\n",
    "}\n",
    "\n",
    "# --- Reuse train/test indices and target ---\n",
    "train_idx = train.index\n",
    "test_idx = test.index\n",
    "y = df_complete['relevance']\n",
    "results = {}\n",
    "\n",
    "# --- Define training/evaluation function ---\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, label):\n",
    "    model = HistGradientBoostingRegressor(max_iter=300, learning_rate=0.1, max_depth=6, random_state=42)\n",
    "    model.fit(X_train.fillna(0), y_train)\n",
    "    preds = model.predict(X_test.fillna(0))\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    results[label] = rmse\n",
    "    print(f\"{label}: RMSE = {rmse:.4f}\")\n",
    "    return model, preds\n",
    "\n",
    "# --- Evaluate each feature set ---\n",
    "for label, features in feature_sets.items():\n",
    "    if not all(f in df_complete.columns for f in features):\n",
    "        print(f\"Skipping {label} (missing columns)\")\n",
    "        continue\n",
    "\n",
    "    X_train = df_complete.loc[train_idx, features]\n",
    "    y_train = y.loc[train_idx].values\n",
    "    X_test = df_complete.loc[test_idx, features]\n",
    "    y_test = y.loc[test_idx].values\n",
    "\n",
    "    train_and_evaluate(X_train, y_train, X_test, y_test, label)\n",
    "\n",
    "# --- Visualize results ---\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(results.keys(), results.values(), color='skyblue')\n",
    "plt.ylabel(\"RMSE (lower is better)\")\n",
    "plt.title(\"HistGradientBoostingRegressor Feature Set Comparison\")\n",
    "plt.xticks(rotation=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
